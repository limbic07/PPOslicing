import os
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor

# å¼•å…¥SLA ç¯å¢ƒ
from env_5g_sla import FiveG_SLA_Env


def make_env():
    """
    Utility function to create environment with Monitor wrapper.
    Monitor wrapper is essential for tracking rewards in TensorBoard.
    """
    env = FiveG_SLA_Env()
    # Monitor ç”¨äºè®°å½•æ¯ä¸€æ­¥çš„æ•°æ®ï¼Œæ–¹ä¾¿ç”»å›¾
    return Monitor(env)


if __name__ == "__main__":
    # --- 1. é…ç½®è·¯å¾„ (Setup Paths) ---
    log_dir = "./logs_formal/"  # TensorBoard æ—¥å¿—ç›®å½•
    models_dir = "./models_formal/"  # æ¨¡å‹ä¿å­˜ç›®å½•

    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)

    # --- 2. åˆ›å»ºç¯å¢ƒ (Create Environments) ---
    # è®­ç»ƒç¯å¢ƒï¼šä½¿ç”¨ VecNormalize è¿›è¡Œå½’ä¸€åŒ–
    # ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæµé‡æ˜¯ 1000Mbpsï¼ŒSLA è¿çº¦æ˜¯ 0/1ï¼Œæ•°å€¼å·®å¼‚å·¨å¤§ï¼Œå½’ä¸€åŒ–èƒ½æå¤§åŠ é€Ÿæ”¶æ•›ã€‚
    env = DummyVecEnv([make_env])
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)

    # è¯„ä¼°ç¯å¢ƒï¼šç”¨äºåœ¨è®­ç»ƒä¸­é€”æµ‹è¯•æ¨¡å‹å¥½å
    # æ³¨æ„ï¼šè¯„ä¼°ç¯å¢ƒä¹Ÿéœ€è¦åŒæ ·çš„å½’ä¸€åŒ–è®¾ç½®
    eval_env = DummyVecEnv([make_env])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)

    # --- 3. å®šä¹‰å›è°ƒå‡½æ•° (Callbacks) ---
    # æ ¸å¿ƒåŠŸèƒ½ï¼šæ¯ 10,000 æ­¥æµ‹è¯•ä¸€æ¬¡ï¼Œå¦‚æœæ•ˆæœæ˜¯å†å²æœ€å¥½çš„ï¼Œå°±ä¿å­˜åˆ° best_model.zip
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=models_dir,
        log_path=log_dir,
        eval_freq=10000,
        deterministic=True,
        render=False,
        n_eval_episodes=10  # æ¯æ¬¡æµ‹è¯•è·‘10ä¸ªå›åˆå–å¹³å‡
    )

    # --- 4. å®šä¹‰ç½‘ç»œæ¶æ„ (Network Architecture) ---
    # é»˜è®¤æ˜¯ [64, 64]ï¼Œå¯¹äº SLA è¿™ç§å¤æ‚é€»è¾‘ï¼Œå»ºè®®åŠ å®½åŠ æ·±åˆ° [256, 256]
    policy_kwargs = dict(
        net_arch=dict(pi=[256, 256], vf=[256, 256])
    )

    # --- 5. åˆå§‹åŒ– PPO æ¨¡å‹ (Init Model) ---
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=0.0003,  # ç»å…¸å­¦ä¹ ç‡ï¼Œå¦‚æœæ³¢åŠ¨å¤§å¯è°ƒå°åˆ° 1e-4
        n_steps=2048,  # æ¯æ¬¡æ›´æ–°é‡‡æ ·çš„æ­¥æ•°
        batch_size=64,  # æ‰¹æ¬¡å¤§å°
        gamma=0.99,  # æŠ˜æ‰£å› å­
        gae_lambda=0.95,  # GAE å‚æ•°
        clip_range=0.2,  # PPO è£å‰ªèŒƒå›´
        policy_kwargs=policy_kwargs,  # ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œ
        tensorboard_log=log_dir,
        device="cpu"  # å¼ºåˆ¶ä½¿ç”¨ CPUï¼Œé€Ÿåº¦æ›´å¿«
    )

    # --- 6. å¼€å§‹æ­£å¼è®­ç»ƒ (Start Training) ---
    print("ğŸš€ Starting Formal Training...")
    print(f"Logs will be saved to: {log_dir}")
    print(f"Best model will be saved to: {models_dir}")

    # æ­¥æ•°ï¼š500,000
    TOTAL_TIMESTEPS = 500,000

    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback)

    # --- 7. ä¿å­˜æœ€ç»ˆç»“æœ (Save Final) ---
    # ä¿å­˜æœ€åçš„æ¨¡å‹
    model.save(f"{models_dir}/final_model")
    # å…³é”®ï¼ä¿å­˜å½’ä¸€åŒ–çš„ç»Ÿè®¡å‚æ•° (å‡å€¼æ–¹å·®)ï¼Œå¦åˆ™å°†æ¥åŠ è½½æ¨¡å‹æ—¶é¢„æµ‹ä¼šä¸å‡†
    env.save(f"{models_dir}/vec_normalize.pkl")

    print("âœ… Training Finished!")